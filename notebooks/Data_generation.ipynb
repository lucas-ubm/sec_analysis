{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name:str) -> dict:\n",
    "    with open(file_name, 'r') as handle:\n",
    "      content = json.load(handle)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = r'C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\extracted_filings'\n",
    "files = glob.glob(files_dir+r'\\*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore previously processed filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_mentions = glob.glob(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions\\*\")\n",
    "# processed_mentions = pd.DataFrame(files, columns=['file_name'])\n",
    "# ai_mentions = glob.glob(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions\\*\")\n",
    "\n",
    "# def mention_exists(file_name:str) -> bool:\n",
    "#     \"\"\"\n",
    "#     Check if a file has been processed\n",
    "#     :param file_name: str\n",
    "#     :return: bool\n",
    "#     \"\"\"\n",
    "#     mention_name = \"C:\\\\Users\\\\P70088982\\\\Documents\\\\edgar-crawler\\\\datasets\\\\ai_mentions\\\\\"+file_name.split('\\\\')[-1].replace('.json', '_matches.json')\n",
    "#     if mention_name in ai_mentions:\n",
    "#         return mention_name\n",
    "#     else:\n",
    "#         return None\n",
    "# processed_mentions['mentions'] = processed_mentions['file_name'].apply(lambda x: mention_exists(x))\n",
    "# # Set processing time to yesterday\n",
    "# processed_mentions['processing_date'] = pd.to_datetime('now', utc=True).date() - pd.DateOffset(days=1)\n",
    "# processed_mentions.to_csv(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\processed_mentions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "original_search_terms = [\"AI\", \"Artificial Intelligence\",\"A.I\", \"Machine Learning\",\"ML\",\"Deep Learning\",\"DL\",\"Neural Network\",\"NLP\",\n",
    "    \"Natural Language Processing\", \"Computer Vision\", \"Robotics\",\n",
    "    \"Computing\",\"algorithm\", \"Chatbot\",\"Recommendation System\", \"Recommender System\", \"Image Recognition\", \"Speech Recognition\", \"Voice Assistant\",\n",
    "    \"Artificial General Intelligence\",\"AGI\", \"artificial\", \"comput\\w*\", \"generative\", \"agent\", \"deepfake\", \"\\w*learning\\w*\", \"autonom\\w*\"]\n",
    "\n",
    "def extract_keywords(file:dict, sections:list=[\"1\", \"1A\", \"3\", \"7\"], search_terms:list=[]):\n",
    "  search_terms = [\"Artificial Intelligence\",\"A\\.I\", \"Machine Learning\",\"Deep Learning\",\"NLP\",\n",
    "    \"Natural Language Processing\", \"Computer Vision\", \"Chatbot\",\"Recommendation System\", \"Recommender System\", \"Image Recognition\", \"Speech Recognition\", \"Voice Assistant\",\n",
    "    \"Artificial General Intelligence\",\"AGI\", \"generative\", \"deepfake\"]\n",
    "  search_pattern = re.compile(r'\\b(' + '|'.join(search_terms) + r')\\b', re.IGNORECASE)\n",
    "  document_results = {}\n",
    "  document_results['company'] = file['company']\n",
    "  document_results['cik'] = file['cik']\n",
    "  document_results['filing_date'] = file['filing_date']\n",
    "  document_results['period_of_report'] = file['period_of_report']\n",
    "  document_results['filename'] = file['filename']\n",
    "\n",
    "  filled=False\n",
    "\n",
    "  for section in sections:\n",
    "      if f'item_{section}' in file:\n",
    "            content = file[f'item_{section}']\n",
    "            matches = list(search_pattern.finditer(content))\n",
    "            for match in matches:\n",
    "                keyword = match.group(0)\n",
    "\n",
    "                # Extract the surrounding sentence\n",
    "                start_index = content.rfind('.', 0, match.start()) + 1\n",
    "                end_index = content.find('.', match.end())\n",
    "                if end_index == -1:  # If no period found, go to the end of the string\n",
    "                    end_index = len(content)\n",
    "                sentence = content[start_index:end_index].strip()\n",
    "\n",
    "                # Extract the paragraph containing the keyword (starting and ending with a newline)\n",
    "                paragraph_start = content.rfind(\"\\n\", 0, match.start()) + 1  # Start of paragraph\n",
    "                paragraph_end = content.find(\"\\n\", match.end())  # End of paragraph\n",
    "                if paragraph_end == -1:  # If no newline is found after the match, go to the end of the string\n",
    "                    paragraph_end = len(content)\n",
    "                paragraph = content[paragraph_start:paragraph_end].strip()\n",
    "\n",
    "                # Append match details\n",
    "                if f'{section}_matches' not in document_results:\n",
    "                    document_results[f'{section}_matches'] = []\n",
    "                    filled=True\n",
    "                document_results[f'{section}_matches'].append({\n",
    "                    \"keyword\": keyword,\n",
    "                    \"sentence\": sentence,\n",
    "                    \"paragraph\": paragraph, \n",
    "                    \"match_id\": f'{file[\"cik\"]}_{section}_{keyword}_{start_index}_{end_index}'})\n",
    "  if not filled:\n",
    "    document_results = None\n",
    "  return document_results            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matches extraction and saving to files \n",
    "Took ~35m for 5 years of data, may take less if files not saved considering everything fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = r'C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\extracted_filings'\n",
    "files = glob.glob(files_dir+r'\\*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mentions = pd.read_csv(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\processed_mentions.csv\").drop(columns=['Unnamed: 0']).set_index('file_name')\n",
    "existing = processed_mentions.index.tolist()\n",
    "to_process = [file for file in files if file not in existing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files processed, 0 files with keywords found, 1406 files remaining\n",
      "100 files processed, 30 files with keywords found, 1306 files remaining\n",
      "200 files processed, 57 files with keywords found, 1206 files remaining\n",
      "300 files processed, 88 files with keywords found, 1106 files remaining\n",
      "400 files processed, 108 files with keywords found, 1006 files remaining\n",
      "500 files processed, 128 files with keywords found, 906 files remaining\n",
      "600 files processed, 156 files with keywords found, 806 files remaining\n",
      "700 files processed, 184 files with keywords found, 706 files remaining\n",
      "800 files processed, 224 files with keywords found, 606 files remaining\n",
      "900 files processed, 253 files with keywords found, 506 files remaining\n",
      "1000 files processed, 270 files with keywords found, 406 files remaining\n",
      "1100 files processed, 288 files with keywords found, 306 files remaining\n",
      "1200 files processed, 303 files with keywords found, 206 files remaining\n",
      "1300 files processed, 328 files with keywords found, 106 files remaining\n",
      "1400 files processed, 360 files with keywords found, 6 files remaining\n"
     ]
    }
   ],
   "source": [
    "json_files = {}\n",
    "# processed_mentions = pd.read_csv(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\processed_mentions.csv\").set_index('file_name')\n",
    "# to_process = [file for file in files if file not in processed_mentions['file_name'].values]\n",
    "for i, file in enumerate(to_process):\n",
    "    example_file = read_file(file)\n",
    "    keywords = extract_keywords(example_file)\n",
    "    processed_mentions.loc[file] = pd.Series([None, pd.to_datetime('now', utc=True).date()], index=processed_mentions.columns)\n",
    "    if keywords:\n",
    "        json_files[file] = keywords\n",
    "        # Save the results to a JSON file\n",
    "        mentions_file = file.replace('.json', '_matches.json').replace('extracted_filings', 'ai_mentions')\n",
    "\n",
    "\n",
    "        with open(mentions_file, 'w') as f:\n",
    "            json.dump(keywords, f, indent=4)\n",
    "        processed_mentions.loc[file, 'mentions'] = mentions_file\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i} files processed, {len(json_files)} files with keywords found, {len(to_process)-i} files remaining\")\n",
    "        # Take out of loop if you don't want to save every 100 files\n",
    "        processed_mentions.to_csv(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\processed_mentions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files loaded, 9092 files remaining\n",
      "100 files loaded, 8992 files remaining\n",
      "200 files loaded, 8892 files remaining\n",
      "300 files loaded, 8792 files remaining\n",
      "400 files loaded, 8692 files remaining\n",
      "500 files loaded, 8592 files remaining\n",
      "600 files loaded, 8492 files remaining\n",
      "700 files loaded, 8392 files remaining\n",
      "800 files loaded, 8292 files remaining\n",
      "900 files loaded, 8192 files remaining\n",
      "1000 files loaded, 8092 files remaining\n",
      "1100 files loaded, 7992 files remaining\n",
      "1200 files loaded, 7892 files remaining\n",
      "1300 files loaded, 7792 files remaining\n",
      "1400 files loaded, 7692 files remaining\n",
      "1500 files loaded, 7592 files remaining\n",
      "1600 files loaded, 7492 files remaining\n",
      "1700 files loaded, 7392 files remaining\n",
      "1800 files loaded, 7292 files remaining\n",
      "1900 files loaded, 7192 files remaining\n",
      "2000 files loaded, 7092 files remaining\n",
      "2100 files loaded, 6992 files remaining\n",
      "2200 files loaded, 6892 files remaining\n",
      "2300 files loaded, 6792 files remaining\n",
      "2400 files loaded, 6692 files remaining\n",
      "2500 files loaded, 6592 files remaining\n",
      "2600 files loaded, 6492 files remaining\n",
      "2700 files loaded, 6392 files remaining\n",
      "2800 files loaded, 6292 files remaining\n",
      "2900 files loaded, 6192 files remaining\n",
      "3000 files loaded, 6092 files remaining\n",
      "3100 files loaded, 5992 files remaining\n",
      "3200 files loaded, 5892 files remaining\n",
      "3300 files loaded, 5792 files remaining\n",
      "3400 files loaded, 5692 files remaining\n",
      "3500 files loaded, 5592 files remaining\n",
      "3600 files loaded, 5492 files remaining\n",
      "3700 files loaded, 5392 files remaining\n",
      "3800 files loaded, 5292 files remaining\n",
      "3900 files loaded, 5192 files remaining\n",
      "4000 files loaded, 5092 files remaining\n",
      "4100 files loaded, 4992 files remaining\n",
      "4200 files loaded, 4892 files remaining\n",
      "4300 files loaded, 4792 files remaining\n",
      "4400 files loaded, 4692 files remaining\n",
      "4500 files loaded, 4592 files remaining\n",
      "4600 files loaded, 4492 files remaining\n",
      "4700 files loaded, 4392 files remaining\n",
      "4800 files loaded, 4292 files remaining\n",
      "4900 files loaded, 4192 files remaining\n",
      "5000 files loaded, 4092 files remaining\n",
      "5100 files loaded, 3992 files remaining\n",
      "5200 files loaded, 3892 files remaining\n",
      "5300 files loaded, 3792 files remaining\n",
      "5400 files loaded, 3692 files remaining\n",
      "5500 files loaded, 3592 files remaining\n",
      "5600 files loaded, 3492 files remaining\n",
      "5700 files loaded, 3392 files remaining\n",
      "5800 files loaded, 3292 files remaining\n",
      "5900 files loaded, 3192 files remaining\n",
      "6000 files loaded, 3092 files remaining\n",
      "6100 files loaded, 2992 files remaining\n",
      "6200 files loaded, 2892 files remaining\n",
      "6300 files loaded, 2792 files remaining\n",
      "6400 files loaded, 2692 files remaining\n",
      "6500 files loaded, 2592 files remaining\n",
      "6600 files loaded, 2492 files remaining\n",
      "6700 files loaded, 2392 files remaining\n",
      "6800 files loaded, 2292 files remaining\n",
      "6900 files loaded, 2192 files remaining\n",
      "7000 files loaded, 2092 files remaining\n",
      "7100 files loaded, 1992 files remaining\n",
      "7200 files loaded, 1892 files remaining\n",
      "7300 files loaded, 1792 files remaining\n",
      "7400 files loaded, 1692 files remaining\n",
      "7500 files loaded, 1592 files remaining\n",
      "7600 files loaded, 1492 files remaining\n",
      "7700 files loaded, 1392 files remaining\n",
      "7800 files loaded, 1292 files remaining\n",
      "7900 files loaded, 1192 files remaining\n",
      "8000 files loaded, 1092 files remaining\n",
      "8100 files loaded, 992 files remaining\n",
      "8200 files loaded, 892 files remaining\n",
      "8300 files loaded, 792 files remaining\n",
      "8400 files loaded, 692 files remaining\n",
      "8500 files loaded, 592 files remaining\n",
      "8600 files loaded, 492 files remaining\n",
      "8700 files loaded, 392 files remaining\n",
      "8800 files loaded, 292 files remaining\n",
      "8900 files loaded, 192 files remaining\n",
      "9000 files loaded, 92 files remaining\n"
     ]
    }
   ],
   "source": [
    "json_files = {}\n",
    "files_paths = glob.glob(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions\\*\")\n",
    "for i, file in enumerate(files_paths):\n",
    "    with open(file, 'r') as handle:\n",
    "        content = json.load(handle)\n",
    "    json_files[file] = content\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i} files loaded, {len(files_paths)-i} files remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "for file in json_files:\n",
    "    match_fields = [match for match in json_files[file].keys() if match.endswith('_matches')]\n",
    "    non_match_fields = [match for match in json_files[file].keys() if not match.endswith('_matches')]\n",
    "    for match_field in match_fields:\n",
    "        for match in json_files[file][match_field]:\n",
    "            row = {}\n",
    "            row.update({field: json_files[file][field] for field in non_match_fields})\n",
    "            row['keyword'] = match['keyword']\n",
    "            row['sentence'] = match['sentence']\n",
    "            row['match_id'] = match['match_id']\n",
    "            row['match_field'] = match_field\n",
    "            df_rows.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df['filing_year'] = pd.to_datetime(matches_df['filing_date']).dt.year \n",
    "matches_df['reporting_year'] = pd.to_datetime(matches_df['period_of_report']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.to_csv(r'C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions.csv', index=False)\n",
    "matches_df.to_excel(r'C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scirpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "def read_file(file_name:str) -> dict:\n",
    "    with open(file_name, 'r') as handle:\n",
    "      content = json.load(handle)\n",
    "    return content\n",
    "files_dir = r'C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\extracted_filings'\n",
    "files = glob.glob(files_dir+r'\\*')\n",
    "\n",
    "# Ignore previously processed filings\n",
    "# ai_mentions = glob.glob(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions\\*\")\n",
    "# processed_mentions = pd.DataFrame(files, columns=['file_name'])\n",
    "# ai_mentions = glob.glob(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions\\*\")\n",
    "\n",
    "# def mention_exists(file_name:str) -> bool:\n",
    "#     \"\"\"\n",
    "#     Check if a file has been processed\n",
    "#     :param file_name: str\n",
    "#     :return: bool\n",
    "#     \"\"\"\n",
    "#     mention_name = \"C:\\\\Users\\\\P70088982\\\\Documents\\\\edgar-crawler\\\\datasets\\\\ai_mentions\\\\\"+file_name.split('\\\\')[-1].replace('.json', '_matches.json')\n",
    "#     if mention_name in ai_mentions:\n",
    "#         return mention_name\n",
    "#     else:\n",
    "#         return None\n",
    "# processed_mentions['mentions'] = processed_mentions['file_name'].apply(lambda x: mention_exists(x))\n",
    "# # Set processing time to yesterday\n",
    "# processed_mentions['processing_date'] = pd.to_datetime('now', utc=True).date() - pd.DateOffset(days=1)\n",
    "# processed_mentions.to_csv(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\processed_mentions.csv\")\n",
    "import re\n",
    "original_search_terms = [\"AI\", \"Artificial Intelligence\",\"A.I\", \"Machine Learning\",\"ML\",\"Deep Learning\",\"DL\",\"Neural Network\",\"NLP\",\n",
    "    \"Natural Language Processing\", \"Computer Vision\", \"Robotics\",\n",
    "    \"Computing\",\"algorithm\", \"Chatbot\",\"Recommendation System\", \"Recommender System\", \"Image Recognition\", \"Speech Recognition\", \"Voice Assistant\",\n",
    "    \"Artificial General Intelligence\",\"AGI\", \"artificial\", \"comput\\w*\", \"generative\", \"agent\", \"deepfake\", \"\\w*learning\\w*\", \"autonom\\w*\"]\n",
    "\n",
    "def extract_keywords(file:dict, sections:list=[\"1\", \"1A\", \"3\", \"7\"], search_terms:list=[]):\n",
    "  search_terms = [\"Artificial Intelligence\",\"A\\.I\", \"Machine Learning\",\"Deep Learning\",\"NLP\",\n",
    "    \"Natural Language Processing\", \"Computer Vision\", \"Chatbot\",\"Recommendation System\", \"Recommender System\", \"Image Recognition\", \"Speech Recognition\", \"Voice Assistant\",\n",
    "    \"Artificial General Intelligence\",\"AGI\", \"generative\", \"deepfake\"]\n",
    "  search_pattern = re.compile(r'\\b(' + '|'.join(search_terms) + r')\\b', re.IGNORECASE)\n",
    "  document_results = {}\n",
    "  document_results['company'] = file['company']\n",
    "  document_results['cik'] = file['cik']\n",
    "  document_results['filing_date'] = file['filing_date']\n",
    "  document_results['period_of_report'] = file['period_of_report']\n",
    "  document_results['filename'] = file['filename']\n",
    "\n",
    "  filled=False\n",
    "\n",
    "  for section in sections:\n",
    "      if f'item_{section}' in file:\n",
    "            content = file[f'item_{section}']\n",
    "            matches = list(search_pattern.finditer(content))\n",
    "            for match in matches:\n",
    "                keyword = match.group(0)\n",
    "\n",
    "                # Extract the surrounding sentence\n",
    "                start_index = content.rfind('.', 0, match.start()) + 1\n",
    "                end_index = content.find('.', match.end())\n",
    "                if end_index == -1:  # If no period found, go to the end of the string\n",
    "                    end_index = len(content)\n",
    "                sentence = content[start_index:end_index].strip()\n",
    "\n",
    "                # Extract the paragraph containing the keyword (starting and ending with a newline)\n",
    "                paragraph_start = content.rfind(\"\\n\", 0, match.start()) + 1  # Start of paragraph\n",
    "                paragraph_end = content.find(\"\\n\", match.end())  # End of paragraph\n",
    "                if paragraph_end == -1:  # If no newline is found after the match, go to the end of the string\n",
    "                    paragraph_end = len(content)\n",
    "                paragraph = content[paragraph_start:paragraph_end].strip()\n",
    "\n",
    "                # Append match details\n",
    "                if f'{section}_matches' not in document_results:\n",
    "                    document_results[f'{section}_matches'] = []\n",
    "                    filled=True\n",
    "                document_results[f'{section}_matches'].append({\n",
    "                    \"keyword\": keyword,\n",
    "                    \"sentence\": sentence,\n",
    "                    \"paragraph\": paragraph, \n",
    "                    \"match_id\": f'{file[\"cik\"]}_{section}_{keyword}_{start_index}_{end_index}'})\n",
    "  if not filled:\n",
    "    document_results = None\n",
    "  return document_results            \n",
    "## Matches extraction and saving to files \n",
    "# Took ~35m for 5 years of data, may take less if files not saved considering everything fit in memory.\n",
    "files_dir = r'C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\extracted_filings'\n",
    "files = glob.glob(files_dir+r'\\*')\n",
    "processed_mentions = pd.read_csv(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\processed_mentions.csv\").drop(columns=['Unnamed: 0']).set_index('file_name')\n",
    "existing = processed_mentions.index.tolist()\n",
    "to_process = [file for file in files if file not in existing]\n",
    "json_files = {}\n",
    "# processed_mentions = pd.read_csv(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\processed_mentions.csv\").set_index('file_name')\n",
    "# to_process = [file for file in files if file not in processed_mentions['file_name'].values]\n",
    "for i, file in enumerate(to_process):\n",
    "    example_file = read_file(file)\n",
    "    keywords = extract_keywords(example_file)\n",
    "    processed_mentions.loc[file] = pd.Series([None, pd.to_datetime('now', utc=True).date()], index=processed_mentions.columns)\n",
    "    if keywords:\n",
    "        json_files[file] = keywords\n",
    "        # Save the results to a JSON file\n",
    "        mentions_file = file.replace('.json', '_matches.json').replace('extracted_filings', 'ai_mentions')\n",
    "\n",
    "\n",
    "        with open(mentions_file, 'w') as f:\n",
    "            json.dump(keywords, f, indent=4)\n",
    "        processed_mentions.loc[file, 'mentions'] = mentions_file\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i} files processed, {len(json_files)} files with keywords found, {len(to_process)-i} files remaining\")\n",
    "        # Take out of loop if you don't want to save every 100 files\n",
    "        processed_mentions.to_csv(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\processed_mentions.csv\")\n",
    "\n",
    "## Dataframe generation\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob \n",
    "\n",
    "json_files = {}\n",
    "files_paths = glob.glob(r\"C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions\\*\")\n",
    "for i, file in enumerate(files_paths):\n",
    "    with open(file, 'r') as handle:\n",
    "        content = json.load(handle)\n",
    "    json_files[file] = content\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i} files loaded, {len(files_paths)-i} files remaining\")\n",
    "df_rows = []\n",
    "for file in json_files:\n",
    "    match_fields = [match for match in json_files[file].keys() if match.endswith('_matches')]\n",
    "    non_match_fields = [match for match in json_files[file].keys() if not match.endswith('_matches')]\n",
    "    for match_field in match_fields:\n",
    "        for match in json_files[file][match_field]:\n",
    "            row = {}\n",
    "            row.update({field: json_files[file][field] for field in non_match_fields})\n",
    "            row['keyword'] = match['keyword']\n",
    "            row['sentence'] = match['sentence']\n",
    "            row['match_id'] = match['match_id']\n",
    "            row['match_field'] = match_field\n",
    "            df_rows.append(row)\n",
    "\n",
    "matches_df = pd.DataFrame(df_rows)\n",
    "matches_df['filing_year'] = pd.to_datetime(matches_df['filing_date']).dt.year \n",
    "matches_df['reporting_year'] = pd.to_datetime(matches_df['period_of_report']).dt.year\n",
    "matches_df.to_csv(r'C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions.csv', index=False)\n",
    "matches_df.to_excel(r'C:\\Users\\P70088982\\Documents\\edgar-crawler\\datasets\\ai_mentions.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edgar_crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
